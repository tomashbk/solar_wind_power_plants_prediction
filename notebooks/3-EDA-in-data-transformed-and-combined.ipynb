{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, PowerTransformer, StandardScaler\n",
    "# from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, GradientBoostingClassifier, VotingClassifier, AdaBoostClassifier, IsolationForest\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from scipy import stats\n",
    "import pyhere\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data_raw = pyhere.here().resolve().joinpath(\"data\", \"raw\")\n",
    "dir_data_interim = pyhere.here().resolve().joinpath(\"data\", \"interim\")\n",
    "dir_data_external = pyhere.here().resolve().joinpath(\"data\", \"external\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATAFRAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_power_plants = pd.read_csv(dir_data_interim/\"power_plants_with_generation_transformed.csv\", index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_transformed = pd.read_csv(dir_data_external/\"v2_transformed_data_combined_with_nasa.csv\", index_col=['index'])\n",
    "df_transformed = pd.read_csv(dir_data_external/\"v3_transformed_data_combined_with_nasa.csv\", index_col=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_power_plants.loc[0:34935, ['capacity_mw', 'primary_fuel_transformed']].index.name = \"index\"\n",
    "csv_power_plants.index.rename('index', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_power_plants_raw = pd.read_csv(dir_data_raw/\"global_power_plant_database.csv\", usecols=['primary_fuel', 'other_fuel1', 'other_fuel2', 'other_fuel3'], engine='python')\n",
    "csv_power_plants = csv_power_plants.join(df_power_plants_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index_set_to_delete = csv_power_plants[csv_power_plants['other_fuel1'].isin(['Solar', 'Wind'])].index.tolist()\n",
    "index_set_to_delete += (csv_power_plants[csv_power_plants['other_fuel2'].isin(['Solar', 'Wind'])].index.tolist())\n",
    "index_set_to_delete += (csv_power_plants[csv_power_plants['other_fuel3'].isin(['Solar', 'Wind'])].index.tolist())\n",
    "\n",
    "csv_power_plants.drop(index_set_to_delete, inplace = True)\n",
    "\n",
    "# csv_power_plants.filter(like='other_fuel').columns\n",
    "\n",
    "# csv_power_plants[csv_power_plants['other_fuel1'].isin(['Solar', 'Wind'])]\n",
    "# csv_power_plants[csv_power_plants['other_fuel2'].isin(['Solar', 'Wind'])]\n",
    "# csv_power_plants[csv_power_plants['other_fuel3'].isin(['Solar', 'Wind'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_combine = [\n",
    "                        'capacity_mw',\n",
    "                        'primary_fuel_transformed',\n",
    "                        # 'other_fuel1',\n",
    "                        # 'other_fuel2',\n",
    "                        # 'other_fuel3',\n",
    "                        'generation_gwh_2013',\n",
    "                        'generation_gwh_2014',\n",
    "                        'generation_gwh_2015',\n",
    "                        'generation_gwh_2016',\n",
    "                        'generation_gwh_2017',\n",
    "                        'generation_gwh_2018',\n",
    "                        'generation_gwh_2019'\n",
    "                    ]\n",
    "# df_transformed_combined = df_transformed.merge(csv_power_plants.loc[0:24360, ['capacity_mw', 'primary_fuel_transformed']],left_on=\"index\", right_on=\"index\")\n",
    "df_transformed_combined = df_transformed.merge(csv_power_plants[columns_to_combine],left_on=\"index\", right_on=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_transformed_combined[['primary_fuel_transformed']].value_counts())\n",
    "df_transformed_combined[['primary_fuel_transformed']].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed_combined.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_transformed_combined = df_transformed_combined.loc[df_transformed_combined['generation_gwh_2013'].notna() & df_transformed_combined['generation_gwh_2014'].notna() & df_transformed_combined['generation_gwh_2015'].notna() & df_transformed_combined['generation_gwh_2016'].notna() & df_transformed_combined['generation_gwh_2017'].notna() & df_transformed_combined['generation_gwh_2018'].notna() & df_transformed_combined['generation_gwh_2019'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_transformed_combined[['primary_fuel_transformed']].value_counts())\n",
    "df_transformed_combined[['primary_fuel_transformed']].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "# df_transformed_combined.isna().sum()\n",
    "# df_transformed_combined[df_transformed_combined.isna().any(axis=1)]\n",
    "# df_transformed_combined.dropna(inplace = True)\n",
    "df_transformed_combined = df_transformed_combined[df_transformed_combined['primary_fuel_transformed'].notna()]\n",
    "df_transformed_combined['code_prim_fuel_transf'] = label_encoder.fit_transform(df_transformed_combined['primary_fuel_transformed'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target_equivalent = df_transformed_combined[['primary_fuel_transformed', 'code_prim_fuel_transf']].drop_duplicates()\n",
    "df_target_equivalent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REMOVING OUTLIERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns_not_consider_outliers = [\n",
    "                                    'capacity_mw',\n",
    "                                    'latitude',\n",
    "                                    'longitude',\n",
    "                                    'primary_fuel_transformed',\n",
    "                                    'code_prim_fuel_transf',\n",
    "                                    'generation_gwh_2013',\n",
    "                                    'generation_gwh_2014',\n",
    "                                    'generation_gwh_2015',\n",
    "                                    'generation_gwh_2016',\n",
    "                                    'generation_gwh_2017',\n",
    "                                    'generation_gwh_2018',\n",
    "                                    'generation_gwh_2019'\n",
    "                                ]\n",
    "# X = X.loc[:,~columns_delete]                        \n",
    "df_transformed_combined_remove_outliers = df_transformed_combined.drop(columns = columns_not_consider_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso = IsolationForest(contamination=0.1)\n",
    "yhat = iso.fit_predict(df_transformed_combined_remove_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select all rows that are outliers\n",
    "mask = yhat == -1\n",
    "index_outliers = df_transformed_combined[mask].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITH OUTLIERS\n",
    "print(df_transformed_combined[['primary_fuel_transformed']].value_counts())\n",
    "df_transformed_combined[['primary_fuel_transformed']].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITHOUT OUTLIERS\n",
    "df_transformed_combined.drop(index_outliers, axis=0, inplace =True)\n",
    "\n",
    "print(df_transformed_combined[['primary_fuel_transformed']].value_counts())\n",
    "df_transformed_combined[['primary_fuel_transformed']].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balancing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def balancing_target(df, target_column):\n",
    "#     target_with_less_values = {\n",
    "#         df_transformed_combined[[target_column]].value_counts().sort_values().index[0][0]: df_transformed_combined[[target_column]].value_counts().sort_values()[0]\n",
    "#     }\n",
    "#     if(target_with_less_values > 1000):\n",
    "\n",
    "\n",
    "# dict_value_counts = df_transformed_combined[['primary_fuel_transformed']].value_counts().sort_values().to_dict()\n",
    "target_with_less_value = df_transformed_combined[['primary_fuel_transformed']].value_counts().sort_values().index[0][0]\n",
    "value_of_target_with_less_value = df_transformed_combined[['primary_fuel_transformed']].value_counts().sort_values()[0]\n",
    "dict_outbalanced_targets_values = df_transformed_combined[['primary_fuel_transformed']].value_counts().sort_values()[1:].to_dict()\n",
    "\n",
    "# next(iter( dict_value_counts.items() ))[0][0]\n",
    "dict_outbalanced_targets_values\n",
    "if(value_of_target_with_less_value > 1000):\n",
    "    # sasa = {key_tuple[0]: (value - value_of_target_with_less_value) for key_tuple, value in dict_outbalanced_targets_values.items()}\n",
    "    # sasa = map(lambda x: x + x, dict_outbalanced_targets_values.items())\n",
    "    for key_tuple, value in dict_outbalanced_targets_values.items():\n",
    "        difference = (value - value_of_target_with_less_value)\n",
    "        index_rows_to_delete = df_transformed_combined[df_transformed_combined['primary_fuel_transformed'] == key_tuple[0]].sample(difference).index\n",
    "        df_transformed_combined.drop(index_rows_to_delete, axis = 0, inplace = True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index_rows_to_delete_other = df_transformed_combined[df_transformed_combined['code_prim_fuel_transf'] == 0].sample(4000).index\n",
    "# index_rows_to_delete_other = df_transformed_combined[df_transformed_combined['code_prim_fuel_transf'] == 0].sample(13500).index\n",
    "# index_rows_to_delete = df_transformed_combined[df_transformed_combined['code_prim_fuel_transf'] == 0].sample(9000).index\n",
    "# index_rows_to_delete_wind = df_transformed_combined[df_transformed_combined['code_prim_fuel_transf'] == 2].sample(100).index\n",
    "# index_rows_to_delete_solar = df_transformed_combined[df_transformed_combined['code_prim_fuel_transf'] == 1].sample(5300).index\n",
    "# index_rows_to_delete_solar = df_transformed_combined[df_transformed_combined['code_prim_fuel_transf'] == 1].sample(3000).index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_transformed_combined.drop(index_rows_to_delete_other, axis = 0, inplace = True)\n",
    "# df_transformed_combined.drop(index_rows_to_delete_solar, axis = 0, inplace = True)\n",
    "# df_transformed_combined.drop(index_rows_to_delete_wind, axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed_combined[['primary_fuel_transformed']].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed_combined.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_transformed_combined.isna().sum()\n",
    "# df_transformed_combined[df_transformed_combined.isna().any(axis=1)]\n",
    "# df_transformed_combined.dropna(inplace = True)\n",
    "columns_to_drop = [\n",
    "                        'capacity_mw',\n",
    "                        'latitude',\n",
    "                        'longitude',\n",
    "                        'primary_fuel_transformed',\n",
    "                        'generation_gwh_2013',\n",
    "                        'generation_gwh_2014',\n",
    "                        'generation_gwh_2015',\n",
    "                        'generation_gwh_2016',\n",
    "                        'generation_gwh_2017',\n",
    "                        'generation_gwh_2018',\n",
    "                        'generation_gwh_2019'\n",
    "                    ]\n",
    "pre_X = df_transformed_combined.drop(columns=columns_to_drop)\n",
    "pre_X.dropna(inplace = True)\n",
    "X = pre_X.drop(columns=['code_prim_fuel_transf'])\n",
    "y = pre_X['code_prim_fuel_transf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.kdeplot(data=pre_X, x='winter_TQV_2009', hue='code_prim_fuel_transf', shade=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mi_scores(X, y):\n",
    "    mi_scores = mutual_info_classif(X, y)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(max_leaf_nodes, X_train, X_test, y_train, y_test):\n",
    "    model = DecisionTreeClassifier(max_leaf_nodes = max_leaf_nodes, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds_val = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, preds_val)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_knn(n_neighbors, X_train, X_test, y_train, y_test):\n",
    "    model = KNeighborsClassifier(n_neighbors = n_neighbors)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds_val = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, preds_val)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mi_scores(scores):\n",
    "    scores = scores.sort_values(ascending=True)\n",
    "    width = np.arange(len(scores))\n",
    "    ticks = list(scores.index)\n",
    "    plt.barh(width, scores)\n",
    "    plt.yticks(width, ticks)\n",
    "    plt.title(\"Mutual Information Scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_matrix(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    A function to calculate and plot\n",
    "    correlation matrix of a DataFrame.\n",
    "    \"\"\"\n",
    "    # Create the matrix\n",
    "    matrix = df.corr()\n",
    "    \n",
    "    # Create cmap\n",
    "    cmap = sns.diverging_palette(250, 15, s=75, l=40,\n",
    "                             n=9, center=\"light\", as_cmap=True)\n",
    "    # Create a mask\n",
    "    mask = np.triu(np.ones_like(matrix, dtype=bool))\n",
    "    \n",
    "    # Make figsize bigger\n",
    "    fig, ax = plt.subplots(figsize=(16,12))\n",
    "    \n",
    "    # Plot the matrix\n",
    "    _ = sns.heatmap(matrix, mask=mask, center=0, annot=True,\n",
    "             fmt='.2f', square=True, cmap=cmap, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation_matrix(X.join(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mi_scores = make_mi_scores(X, y)\n",
    "# mi_scores[::3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mi_scores[:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=100, figsize=(8, 30))\n",
    "plot_mi_scores(mi_scores[500:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANGE\n",
    "# ANN\n",
    "# ALLSKY_SFC_SW_UP\n",
    "# CLRSKY_SFC_SW_UP\n",
    "# SW_DIFF\n",
    "# columns_delete = X.columns.str.contains('SW_DIFF') | X.columns.str.contains('RANGE') | X.columns.str.contains('ALLSKY_SFC_SW_UP') |  X.columns.str.contains('CLRSKY_SFC_SW_UP') | X.columns.str.contains('ANN') | X.columns.str.contains('2019') | X.columns.str.contains('2013') | X.columns.str.contains('2014') | X.columns.str.contains('2015') | X.columns.str.contains('2016') | X.columns.str.contains('2017') \n",
    "columns_delete = X.columns.str.contains('SW_DIFF') | X.columns.str.contains('RANGE') | X.columns.str.contains('ALLSKY_SFC_SW_UP') |  X.columns.str.contains('CLRSKY_SFC_SW_UP') | X.columns.str.contains('ANN') \n",
    "X = X.loc[:,~columns_delete]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_scores = make_mi_scores(X, y)\n",
    "plt.figure(dpi=100, figsize=(8, 30))\n",
    "plot_mi_scores(mi_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIMATE_FEATURES = {'ALLSKY_SFC_SW_DWN', \n",
    "#                     'CLRSKY_SFC_SW_DWN', \n",
    "#                     'ALLSKY_KT', \n",
    "#                     'WS10M_MIN_AVG', \n",
    "#                     'WS10M_MAX_AVG', \n",
    "#                     'WS50M_MAX_AVG', \n",
    "#                     'WS50M_MIN_AVG'} \n",
    "CLIMATE_FEATURES = {\"ALLSKY_SFC_SW_DWN\",\n",
    "                        \"CLRSKY_SFC_SW_DWN\",\n",
    "                        # \"ALLSKY_SFC_SW_DIFF\",\n",
    "                        # \"ALLSKY_SFC_SW_UP\",\n",
    "                        \"ALLSKY_SFC_LW_DWN\",\n",
    "                        \"ALLSKY_SFC_LW_UP\",\n",
    "                        \"ALLSKY_SFC_SW_DNI\",\n",
    "                        # \"ALLSKY_SFC_SW_DNI_MAX_RD\",\n",
    "                        # \"ALLSKY_SFC_SW_UP_MAX\",\n",
    "                        # \"CLRSKY_SFC_SW_DIFF\",\n",
    "                        \"CLRSKY_SFC_SW_DNI\",\n",
    "                        # \"CLRSKY_SFC_SW_UP\",\n",
    "                        #\"ALLSKY_KT\",\n",
    "                        \"WS10M_MAX_AVG\",\n",
    "                        \"WS50M_MAX_AVG\",\n",
    "                        \"WS50M\",\n",
    "                        # \"WS50M_RANGE_AVG\",\n",
    "                        \"WS10M\",\n",
    "                        # \"WS10M_RANGE_AVG\"\n",
    "                    }   \n",
    "SEASONS = {'autumn', 'spring', 'summer', 'winter'}\n",
    "YEARS = {'2012', '2013', '2014', '2015', '2016', '2017', '2018'}\n",
    "\n",
    "list_total = []\n",
    "for feature in CLIMATE_FEATURES:\n",
    "    feature_string = \"\"\n",
    "    \n",
    "    for season in SEASONS:\n",
    "        dict_features_to_apply_mean = {}\n",
    "        list_to_append = []\n",
    "        for year in YEARS:\n",
    "            feature_string = f\"{season}_{feature}_{year}\"\n",
    "            \n",
    "            list_to_append.append(feature_string)\n",
    "        dict_features_to_apply_mean = {f\"{season}_{feature}\": list_to_append}\n",
    "        # print(\"sasa\")\n",
    "        # print(dict_features_to_apply_mean)\n",
    "    \n",
    "        list_total.append(dict_features_to_apply_mean)\n",
    "print(list_total)\n",
    "# X[['autumn_ALLSKY_SFC_SW_DWN_2012', 'salary_3']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dict_season_feature in list_total:\n",
    "    for season_feature in dict_season_feature:\n",
    "        print(dict_season_feature[season_feature])\n",
    "        X[f'mean_{season_feature}']= X[dict_season_feature[season_feature]].mean(axis=1)\n",
    "        X[f'std_{season_feature}']= X[dict_season_feature[season_feature]].std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  columns_delete = X.columns.str.contains('ALLSKY_KT') | X.columns.str.contains('ANN') | X.columns.str.contains('autumn_CLRSKY_SFC_SW_DWN_2019')\n",
    "# columns_delete = X.columns.str.contains('ALLSKY_KT') | X.columns.str.contains('ANN') | X.columns.str.contains('2019') | X.columns.str.contains('2013') | X.columns.str.contains('2014') | X.columns.str.contains('2015') | X.columns.str.contains('2016') | X.columns.str.contains('2017') | X.columns.str.contains('MIN')\n",
    "# columns_delete = X.columns.str.contains('ALLSKY_KT') | X.columns.str.contains('2019') | X.columns.str.contains('2012') | X.columns.str.contains('2013') | X.columns.str.contains('2014') | X.columns.str.contains('2015') | X.columns.str.contains('2016') | X.columns.str.contains('2017') | X.columns.str.contains('2018') | X.columns.str.contains('MIN') | X.columns.str.contains('ANN')\n",
    "# columns_delete = X.columns.str.contains('2019') | X.columns.str.contains('2013') | X.columns.str.contains('2014') | X.columns.str.contains('2015') | X.columns.str.contains('2016') | X.columns.str.contains('2017') | X.columns.str.contains('2018') | X.columns.str.contains('ANN') #| X.columns.str.contains('WS50M_RANGE_AVG') | X.columns.str.contains('WS10M_RANGE_AVG') | X.columns.str.contains('CLRSKY_SFC_SW_UP') | X.columns.str.contains('CLRSKY_SFC_SW_DNI') | X.columns.str.contains('CLRSKY_SFC_SW_DIFF') | X.columns.str.contains('ALLSKY_SFC_SW_UP_MAX') | X.columns.str.contains('ALLSKY_SFC_SW_DNI') | X.columns.str.contains('ALLSKY_SFC_LW_UP') | X.columns.str.contains('ALLSKY_SFC_LW_DWN') | X.columns.str.contains('ALLSKY_SFC_SW_UP') | X.columns.str.contains('ALLSKY_SFC_SW_DIFF')\n",
    "columns_delete = X.columns.str.contains('2019') | X.columns.str.contains('2012') | X.columns.str.contains('2013') | X.columns.str.contains('2014') | X.columns.str.contains('2015') | X.columns.str.contains('2016') | X.columns.str.contains('2017') | X.columns.str.contains('2018') | X.columns.str.contains('ANN')\n",
    "# columns_delete = X.columns.str.contains('ANN') | X.columns.str.contains('2019') | X.columns.str.contains('2013') | X.columns.str.contains('2014') | X.columns.str.contains('2015') | X.columns.str.contains('2016') | X.columns.str.contains('2017') \n",
    "X = X.loc[:,~columns_delete]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_scores = make_mi_scores(X, y)\n",
    "plt.figure(dpi=100, figsize=(8, 30))\n",
    "plot_mi_scores(mi_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix(X.join(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.2,random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DECISION TREE CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_max_leaf_nodes = [10, 50, 80, 100, 500, 1200, 1500]\n",
    "results = {}\n",
    "# Write loop to find the ideal tree size from candidate_max_leaf_nodes\n",
    "\n",
    "\n",
    "results = {node: get_accuracy(node, train_X, val_X, train_y, val_y) for node in candidate_max_leaf_nodes}\n",
    "# results = {node: get_accuracy(node, train_X_scaled_pca, val_X_scaled_pca, train_y, val_y) for node in candidate_max_leaf_nodes}\n",
    "# for node in candidate_max_leaf_nodes:\n",
    "#     results[node] = get_accuracy(node, X_train, X_test, y_train, y_test)\n",
    "# Store the best value of max_leaf_nodes (it will be either 5, 25, 50, 100, 250 or 500)\n",
    "best_tree_size = max(results, key=results.get)\n",
    "print(best_tree_size)\n",
    "sns.lineplot(data=results, x= results.keys(), y= results.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model = DecisionTreeClassifier(max_leaf_nodes = best_tree_size, random_state=0)\n",
    "tree_model.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tree_model.predict(val_X)\n",
    "print(tree_model.score(val_X, val_y))\n",
    "results_cvs = cross_val_score(tree_model, X, y, cv=StratifiedKFold(shuffle = True))\n",
    "print(results_cvs)\n",
    "print(f'Mean: {results_cvs.mean()}, Standard Deviation: {results_cvs.std()}')\n",
    "confusion_matrix_return = confusion_matrix(val_y, y_pred)\n",
    "sns.heatmap(confusion_matrix_return, annot=True, fmt = 'g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM FOREST CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numbers of trees\n",
    "n_estimators = [int(x) for x in np.arange(10, 101, 10)]\n",
    "# Numbers of features to consider at every split\n",
    "# max_features = [1, \"sqrt\", \"log2\"]\n",
    "# Maximum numbers of levels in tree\n",
    "max_depth = [50, 100, 500]\n",
    "# Minimum numbers of samples required to split a node\n",
    "min_samples_split = [2, 10, 50, ]\n",
    "# Minimum numbers of samples required at each leaf node\n",
    "min_samples_leaf = [1, 10, 50, 100]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "max_leaf_nodes = [10, 100, 500]\n",
    "\n",
    "param_grid = {\n",
    "                'n_estimators': n_estimators,\n",
    "                # 'max_features': max_features,\n",
    "                'max_depth': max_depth,\n",
    "                'min_samples_split': min_samples_split,\n",
    "                'min_samples_leaf': min_samples_leaf,\n",
    "                'bootstrap': bootstrap,\n",
    "                'max_leaf_nodes': max_leaf_nodes\n",
    "            }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(random_state=0)\n",
    "# rf_grid = GridSearchCV(estimator = rf_model, param_grid = param_grid, cv=5, verbose = 2, n_jobs=4)\n",
    "rf_grid = RandomizedSearchCV(estimator = rf_model, param_distributions = param_grid, n_iter = 10, cv=5, verbose = 2, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_grid.fit(train_X_scaled_pca, train_y)\n",
    "rf_grid.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid.score(val_X, val_y)\n",
    "# rf_grid.score(val_X_scaled_pca, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model2 = RandomForestClassifier(bootstrap= False,\n",
    " max_depth= 50,\n",
    " min_samples_leaf= 1,\n",
    " min_samples_split= 50,\n",
    " n_estimators= 60,\n",
    " max_leaf_nodes= 500,\n",
    " random_state=0)\n",
    "rf_model2.fit(train_X, train_y)\n",
    "y_pred = rf_model2.predict(val_X)\n",
    "# print(cross_val_score(tree_model, X, y, cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(rf_model2.score(val_X, val_y))\n",
    "results_cvs = cross_val_score(rf_model2, X, y, cv=StratifiedKFold(shuffle = True))\n",
    "print(results_cvs)\n",
    "print(f'Mean: {results_cvs.mean()}, Standard Deviation: {results_cvs.std()}')\n",
    "confusion_matrix_return = confusion_matrix(val_y, y_pred)\n",
    "sns.heatmap(confusion_matrix_return, annot=True, fmt = 'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(val_y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model2.score(val_X, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_y.value_counts())\n",
    "val_y.value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbclass_model_1 = XGBClassifier(random_state=0)\n",
    "xgbclass_model_1.fit(train_X, train_y)\n",
    "# xgbclass_model_1.fit(train_X_scaled_pca, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgbclass_model_1.predict(val_X)\n",
    "print(np.around(xgbclass_model_1.score(val_X, val_y),3))\n",
    "# If data is unordered in nature (i.e. non - Time series) then shuffle = True is right choice.\n",
    "results_cvs = cross_val_score(xgbclass_model_1, X, y, cv=StratifiedKFold(shuffle = True))\n",
    "print(np.around(results_cvs, 3))\n",
    "print(f'Mean: {np.around(results_cvs.mean(),3)}, Standard Deviation: {np.around(results_cvs.std(),3)}')\n",
    "confusion_matrix_return = confusion_matrix(val_y, y_pred)\n",
    "sns.heatmap(confusion_matrix_return, annot=True, fmt = 'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "                'n_estimators': [5,10,50,100,500,1000],\n",
    "                'learning_rate': [0.01,0.05,0.1,0.2],\n",
    "                'max_depth': [2,3,4],\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_grid = GridSearchCV(estimator = XGBClassifier(), param_grid = param_grid, cv=5, verbose = 2, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_grid.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_grid.score(val_X, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbclass_model_2 = XGBClassifier(n_estimators=500, learning_rate=0.05, n_jobs=4)\n",
    "# xgbclass_model_2 = XGBClassifier(n_estimators=1000, learning_rate=0.05, n_jobs=4, early_stopping_rounds=5)\n",
    "xgbclass_model_2.fit(train_X, train_y, \n",
    "             eval_set=[(val_X, val_y)], \n",
    "             verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgbclass_model_2.predict(val_X)\n",
    "print(np.around(xgbclass_model_2.score(val_X, val_y),3))\n",
    "# If data is unordered in nature (i.e. non - Time series) then shuffle = True is right choice.\n",
    "results_cvs = cross_val_score(xgbclass_model_2, X, y, cv=StratifiedKFold(shuffle = True))\n",
    "print(np.around(results_cvs, 3))\n",
    "print(f'Mean: {np.around(results_cvs.mean(),3)}, Standard Deviation: {np.around(results_cvs.std(),3)}')\n",
    "confusion_matrix_return = confusion_matrix(val_y, y_pred)\n",
    "sns.heatmap(confusion_matrix_return, annot=True, fmt = 'g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_n_neighbors = np.arange(1,31)\n",
    "results = {}\n",
    "# Write loop to find the ideal tree size from candidate_max_leaf_nodes\n",
    "\n",
    "\n",
    "# results = {n: get_accuracy_knn(n, train_X_scaled_pca, val_X_scaled_pca, train_y, val_y) for n in candidate_n_neighbors}\n",
    "results = {n: get_accuracy_knn(n, train_X, val_X, train_y, val_y) for n in candidate_n_neighbors}\n",
    "best_n_neighbors = max(results, key=results.get)\n",
    "print(best_n_neighbors)\n",
    "sns.lineplot(data=results, x= results.keys(), y= results.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model_1 = KNeighborsClassifier(n_neighbors = best_n_neighbors)\n",
    "knn_model_1.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn_model_1.predict(val_X)\n",
    "print(np.around(knn_model_1.score(val_X, val_y),3))\n",
    "# If data is unordered in nature (i.e. non - Time series) then shuffle = True is right choice.\n",
    "results_cvs = cross_val_score(knn_model_1, X, y, cv=StratifiedKFold(shuffle = True))\n",
    "print(np.around(results_cvs, 3))\n",
    "print(f'Mean: {np.around(results_cvs.mean(),3)}, Standard Deviation: {np.around(results_cvs.std(),3)}')\n",
    "confusion_matrix_return = confusion_matrix(val_y, y_pred)\n",
    "sns.heatmap(confusion_matrix_return, annot=True, fmt = 'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_X_normalized = stats.boxcox(train_X)\n",
    "x = train_X.values #returns a numpy array\n",
    "min_max_scaler = MinMaxScaler()\n",
    "power_transformer = PowerTransformer(method='yeo-johnson')\n",
    "standard_scaler = StandardScaler()\n",
    "# x_scaled = min_max_scaler.fit_transform(x)\n",
    "# x_scaled = power_transformer.fit_transform(x)\n",
    "x_scaled = standard_scaler.fit_transform(x)\n",
    "train_X_scaled = pd.DataFrame(x_scaled, index=train_X.index, columns=train_X.columns)\n",
    "\n",
    "x = val_X.values #returns a numpy array\n",
    "# x_scaled = min_max_scaler.fit_transform(x)\n",
    "# x_scaled = power_transformer.fit_transform(x)\n",
    "x_scaled = standard_scaler.fit_transform(x)\n",
    "val_X_scaled = pd.DataFrame(x_scaled, index=val_X.index, columns=val_X.columns)\n",
    "\n",
    "x = X.values #returns a numpy array\n",
    "# x_scaled = min_max_scaler.fit_transform(x)\n",
    "# x_scaled = power_transformer.fit_transform(x)\n",
    "x_scaled = standard_scaler.fit_transform(x)\n",
    "X_scaled = pd.DataFrame(x_scaled, index=X.index, columns=X.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def axis_calculation(total_columns_df, grid_columns):\n",
    "    rows = int(total_columns_df / grid_columns)\n",
    "    if( total_columns_df % grid_columns) > 0:\n",
    "        rows += 1\n",
    "\n",
    "    return (rows, grid_columns)\n",
    "print(axis_calculation(30, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # plt.figure(figsize=(200,200))\n",
    "# grid_columns = 5\n",
    "# result_axis = axis_calculation(len(train_X_scaled.columns), grid_columns)\n",
    "# fig, axs = plt.subplots(result_axis[0],result_axis[1], figsize=(30,20))\n",
    "# fig.subplots_adjust(hspace=.5)\n",
    "\n",
    "# aux_cont = 0\n",
    "# aux_cont_2 = 0\n",
    "# # fig.figsize=(30,30)\n",
    "# for col in train_X_scaled.columns:\n",
    "# # #     # print(col)\n",
    "#     sns.kdeplot(data=train_X_scaled, x=col, shade=True, ax=axs[aux_cont_2, aux_cont] )\n",
    "#     # axs[aux_cont_2, aux_cont].set_xlabel('Participation Rate')\n",
    "#     # axs[aux_cont_2, aux_cont].set_ylabel('Frequency')\n",
    "#     # axs[aux_cont_2, aux_cont].set_title(\"sasa\")\n",
    "#     axs[aux_cont_2, aux_cont].set_xticklabels([])\n",
    "#     aux_cont += 1\n",
    "#     if aux_cont >= grid_columns:\n",
    "#         aux_cont_2 +=1\n",
    "#         aux_cont = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_X_scaled.hist(bins=100, xlabelsize=8, ylabelsize=10, figsize=(30,20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITHOUT SCALING OR NORMALIZING\n",
    "# train_X.hist(bins=100, xlabelsize=8, ylabelsize=10, figsize=(30,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_n_neighbors = np.arange(1,25)\n",
    "results = {}\n",
    "# Write loop to find the ideal tree size from candidate_max_leaf_nodes\n",
    "\n",
    "\n",
    "results = {n: get_accuracy_knn(n, train_X_scaled, val_X_scaled, train_y, val_y) for n in candidate_n_neighbors}\n",
    "best_n_neighbors = max(results, key=results.get)\n",
    "print(best_n_neighbors)\n",
    "sns.lineplot(data=results, x= results.keys(), y= results.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model_2 = KNeighborsClassifier(n_neighbors = best_n_neighbors)\n",
    "knn_model_2.fit(train_X_scaled, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model_2.score(val_X_scaled, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn_model_2.predict(val_X_scaled)\n",
    "print(np.around(knn_model_2.score(val_X_scaled, val_y),3))\n",
    "# If data is unordered in nature (i.e. non - Time series) then shuffle = True is right choice.\n",
    "results_cvs = cross_val_score(knn_model_2, X_scaled, y, cv=StratifiedKFold(shuffle = True))\n",
    "print(np.around(results_cvs, 3))\n",
    "print(f'Mean: {np.around(results_cvs.mean(),3)}, Standard Deviation: {np.around(results_cvs.std(),3)}')\n",
    "confusion_matrix_return = confusion_matrix(val_y, y_pred)\n",
    "sns.heatmap(confusion_matrix_return, annot=True, fmt = 'g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model_1 = GaussianNB()\n",
    "\n",
    "#Entreno el modelo\n",
    "nb_model_1.fit(train_X, train_y)\n",
    "\n",
    "#Realizo una predicción\n",
    "y_pred = nb_model_1.predict(val_X_scaled)\n",
    "print(np.around(nb_model_1.score(val_X, val_y),3))\n",
    "# If data is unordered in nature (i.e. non - Time series) then shuffle = True is right choice.\n",
    "results_cvs = cross_val_score(nb_model_1, X, y, cv=StratifiedKFold(shuffle = True))\n",
    "print(np.around(results_cvs, 3))\n",
    "print(f'Mean: {np.around(results_cvs.mean(),3)}, Standard Deviation: {np.around(results_cvs.std(),3)}')\n",
    "confusion_matrix_return = confusion_matrix(val_y, y_pred)\n",
    "sns.heatmap(confusion_matrix_return, annot=True, fmt = 'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model_2 = GaussianNB()\n",
    "nb_model_2.fit(train_X_scaled, train_y)\n",
    "\n",
    "\n",
    "y_pred = nb_model_2.predict(val_X_scaled)\n",
    "print(np.around(nb_model_2.score(val_X_scaled, val_y),3))\n",
    "# If data is unordered in nature (i.e. non - Time series) then shuffle = True is right choice.\n",
    "results_cvs = cross_val_score(nb_model_2, X_scaled, y, cv=StratifiedKFold(shuffle = True))\n",
    "print(np.around(results_cvs, 3))\n",
    "print(f'Mean: {np.around(results_cvs.mean(),3)}, Standard Deviation: {np.around(results_cvs.std(),3)}')\n",
    "confusion_matrix_return = confusion_matrix(val_y, y_pred)\n",
    "sns.heatmap(confusion_matrix_return, annot=True, fmt = 'g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model_1 = SVC(random_state=22)\n",
    "svm_model_1.fit(train_X_scaled, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm_model_1.predict(val_X_scaled)\n",
    "print(np.around(svm_model_1.score(val_X_scaled, val_y),3))\n",
    "# If data is unordered in nature (i.e. non - Time series) then shuffle = True is right choice.\n",
    "results_cvs = cross_val_score(svm_model_1, X_scaled, y, cv=StratifiedKFold(shuffle = True))\n",
    "print(np.around(results_cvs, 3))\n",
    "print(f'Mean: {np.around(results_cvs.mean(),3)}, Standard Deviation: {np.around(results_cvs.std(),3)}')\n",
    "confusion_matrix_return = confusion_matrix(val_y, y_pred)\n",
    "sns.heatmap(confusion_matrix_return, annot=True, fmt = 'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "                'C': [0.5, 1, 10, 100], # NOTE: values for C must be > 0\n",
    "                'gamma': ['scale', 1, .1, .01, .001, .0001],\n",
    "                'kernel': ['rbf'] # Some linear fittings last to 7/8 mins. Not recommended at all\n",
    "            }\n",
    "# svm_grid = GridSearchCV(estimator = SVC(), param_grid = param_grid, cv=5, verbose = 2, n_jobs=4)\n",
    "svm_grid = RandomizedSearchCV(estimator = SVC(), param_distributions = param_grid, cv=5, verbose = 2, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_grid.fit(train_X_scaled, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model_2 = SVC(C= 100, gamma= 1, kernel='rbf', random_state=22)\n",
    "svm_model_2.fit(train_X_scaled, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm_model_2.predict(val_X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model_2.score(val_X_scaled, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_return = confusion_matrix(val_y, y_pred)\n",
    "sns.heatmap(confusion_matrix_return, annot=True, fmt = 'g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "pca.fit(train_X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PC_values = np.arange(pca.n_components_) + 1\n",
    "plt.plot(PC_values, pca.explained_variance_ratio_, 'o-', linewidth=2, color='blue')\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Variance Explained')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_scaled_pca = pca.transform(train_X_scaled)\n",
    "val_X_scaled_pca = pca.transform(val_X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pc1_coords = train_X_scaled_pca[:,0]\n",
    "train_pc2_coords = train_X_scaled_pca[:,1]\n",
    "train_pc3_coords = train_X_scaled_pca[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model_1 = LogisticRegression(random_state=22)\n",
    "logistic_model_1.fit(train_X_scaled, train_y)\n",
    "y_pred = logistic_model_1.predict(val_X_scaled)\n",
    "print(logistic_model_1.score(val_X_scaled, val_y))\n",
    "confusion_matrix_return = confusion_matrix(val_y, y_pred)\n",
    "sns.heatmap(confusion_matrix_return, annot=True, fmt = 'g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA(n_components = 2)\n",
    "train_X_scaled_lda = lda.fit_transform(train_X_scaled, train_y)\n",
    "val_X_scaled_lda = lda.transform(val_X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.fit(train_X_scaled_lda, train_y)\n",
    "y_pred = lda.predict(val_X_scaled_lda)\n",
    "print(lda.score(val_X_scaled_lda, val_y))\n",
    "confusion_matrix_return = confusion_matrix(val_y, y_pred)\n",
    "sns.heatmap(confusion_matrix_return, annot=True, fmt = 'g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model_1 = MLPClassifier(hidden_layer_sizes=(25,50,25), max_iter=200,activation = 'tanh',solver='adam',random_state=1)\n",
    "nn_model_1.fit(train_X, train_y)\n",
    "y_pred = nn_model_1.predict(val_X)\n",
    "print(nn_model_1.score(val_X, val_y))\n",
    "confusion_matrix_return = confusion_matrix(val_y, y_pred)\n",
    "sns.heatmap(confusion_matrix_return, annot=True, fmt = 'g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_predict = pd.read_csv(dir_data_external/\"data_predict.csv\", index_col=['index'])\n",
    "df_to_predict.drop(columns=['latitude','longitude'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option(\"display.max_columns\", None)\n",
    "# pd.set_option(\"display.max_rows\", None)\n",
    "# pd.options.display.max_columns = None\n",
    "# pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_to_predict.columns)\n",
    "# print(df_to_predict.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns_delete = df_to_predict.columns.str.contains('ALLSKY_KT') | df_to_predict.columns.str.contains('ANN') | df_to_predict.columns.str.contains('autumn_CLRSKY_SFC_SW_DWN_2019')\n",
    "columns_delete = df_to_predict.columns.str.contains('ALLSKY_KT') | df_to_predict.columns.str.contains('ANN') | df_to_predict.columns.str.contains('2019') | df_to_predict.columns.str.contains('2013') | df_to_predict.columns.str.contains('2014') | df_to_predict.columns.str.contains('2015') | df_to_predict.columns.str.contains('2016') | df_to_predict.columns.str.contains('2017') | df_to_predict.columns.str.contains('MIN')\n",
    "df_to_predict = df_to_predict.loc[:,~columns_delete]\n",
    "# df_to_predict = df_to_predict.loc[:,~COLUMNS_TO_DELETE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_test = xgbclass_model_1.predict(df_to_predict)\n",
    "print(y_predicted_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target_equivalent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAGGING CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_model_1 = BaggingClassifier(base_estimator=KNeighborsClassifier(), n_estimators=50)\n",
    "bag_model_1.fit(train_X_scaled, train_y)\n",
    "y_pred = bag_model_1.predict(val_X_scaled)\n",
    "# print(bag_model_1.score(val_X_scaled, val_y))\n",
    "\n",
    "# results_cvs = cross_val_score(bag_model_1, train_X_scaled, train_y, cv=5)\n",
    "# print(results_cvs)\n",
    "# print(f'Mean: {results_cvs.mean()}, Standard Deviation: {results_cvs.std()}')\n",
    "# confusion_matrix_return = confusion_matrix(val_y, y_pred)\n",
    "# sns.heatmap(confusion_matrix_return, annot=True, fmt = 'g')\n",
    "\n",
    "\n",
    "print(np.around(bag_model_1.score(val_X_scaled, val_y),3))\n",
    "# If data is unordered in nature (i.e. non - Time series) then shuffle = True is right choice.\n",
    "results_cvs = cross_val_score(bag_model_1, X_scaled, y, cv=StratifiedKFold(shuffle = True))\n",
    "print(np.around(results_cvs, 3))\n",
    "print(f'Mean: {np.around(results_cvs.mean(),3)}, Standard Deviation: {np.around(results_cvs.std(),3)}')\n",
    "confusion_matrix_return = confusion_matrix(val_y, y_pred)\n",
    "sns.heatmap(confusion_matrix_return, annot=True, fmt = 'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = {\n",
    "        'KNeighbors': KNeighborsClassifier(n_neighbors=9),\n",
    "        'DecisionTree': DecisionTreeClassifier(max_leaf_nodes=50),\n",
    "        'XGBClassifier': XGBClassifier()\n",
    "    }\n",
    "\n",
    "for name, estimator in classifier.items():\n",
    "    bag_class = BaggingClassifier(base_estimator=estimator, n_estimators=5).fit(train_X_scaled, train_y)\n",
    "    \n",
    "    bag_pred = bag_class.predict(val_X_scaled)\n",
    "\n",
    "    print(f'Accuracy Bagging with {name}: {accuracy_score(bag_pred, val_y)}')\n",
    "    print('')\n",
    "    print(f'Mean: {cross_val_score(bag_class, X, y, cv=5).mean()}, Standard Deviation: {cross_val_score(tree_model, X, y, cv=5).std()}')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VOTING CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "votclass_model_1 = VotingClassifier(estimators=[\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=9)), ('dt', DecisionTreeClassifier(max_leaf_nodes=50)), ('xgb', XGBClassifier())], voting='hard')\n",
    "votclass_model_1.fit(train_X_scaled, train_y)\n",
    "\n",
    "y_pred = votclass_model_1.predict(val_X_scaled)\n",
    "\n",
    "print(np.around(votclass_model_1.score(val_X_scaled, val_y),3))\n",
    "# If data is unordered in nature (i.e. non - Time series) then shuffle = True is right choice.\n",
    "results_cvs = cross_val_score(votclass_model_1, X_scaled, y, cv=StratifiedKFold(shuffle = True))\n",
    "print(np.around(results_cvs, 3))\n",
    "print(f'Mean: {np.around(results_cvs.mean(),3)}, Standard Deviation: {np.around(results_cvs.std(),3)}')\n",
    "confusion_matrix_return = confusion_matrix(val_y, y_pred)\n",
    "sns.heatmap(confusion_matrix_return, annot=True, fmt = 'g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADABOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaBoost_model_1 = AdaBoostClassifier(n_estimators=100)\n",
    "adaBoost_model_1.fit(train_X_scaled, train_y)\n",
    "\n",
    "y_pred = adaBoost_model_1.predict(val_X_scaled)\n",
    "\n",
    "print(np.around(adaBoost_model_1.score(val_X_scaled, val_y),3))\n",
    "# If data is unordered in nature (i.e. non - Time series) then shuffle = True is right choice.\n",
    "results_cvs = cross_val_score(adaBoost_model_1, X_scaled, y, cv=StratifiedKFold(shuffle = True))\n",
    "print(np.around(results_cvs, 3))\n",
    "print(f'Mean: {np.around(results_cvs.mean(),3)}, Standard Deviation: {np.around(results_cvs.std(),3)}')\n",
    "confusion_matrix_return = confusion_matrix(val_y, y_pred)\n",
    "sns.heatmap(confusion_matrix_return, annot=True, fmt = 'g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRADIENT TREE BOOSTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradboost_model_1 = GradientBoostingClassifier(n_estimators=40)\n",
    "gradboost_model_1.fit(train_X, train_y)\n",
    "y_pred = gradboost_model_1.predict(val_X)\n",
    "print(np.around(gradboost_model_1.score(val_X, val_y),3))\n",
    "# If data is unordered in nature (i.e. non - Time series) then shuffle = True is right choice.\n",
    "results_cvs = cross_val_score(gradboost_model_1, X, y, cv=StratifiedKFold(shuffle = True))\n",
    "print(np.around(results_cvs, 3))\n",
    "print(f'Mean: {np.around(results_cvs.mean(),3)}, Standard Deviation: {np.around(results_cvs.std(),3)}')\n",
    "confusion_matrix_return = confusion_matrix(val_y, y_pred)\n",
    "sns.heatmap(confusion_matrix_return, annot=True, fmt = 'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "33e38b4fc567f4e8019f05d798cf38d5465d2749ead22ae1a365825ed0d2a079"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env_renewable_power_plants_pred': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
